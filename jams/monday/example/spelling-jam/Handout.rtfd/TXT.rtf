{\rtf1\ansi\ansicpg1252\cocoartf1187\cocoasubrtf390
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;\f1\fswiss\fcharset0 Helvetica;\f2\froman\fcharset0 Times-Roman;
}
{\colortbl;\red255\green255\blue255;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww18460\viewh13520\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 """Spelling Corrector.\
\
Copyright 2007 Peter Norvig. \
\
Open source code under MIT license: http://www.opensource.org/licenses/mit-license.php\
"""\
\
import re, collections\
\
def words(text): return re.findall('[a-z]+', text.lower()) \
\
def train(features):\
    model = collections.defaultdict(lambda: 1)\
    for f in features:\
        model[f] += 1\
    return model\
\
NWORDS = train(words(file('big.txt').read()))\
\
alphabet = 'abcdefghijklmnopqrstuvwxyz'\
\
def edits1(word):\
   s = [(word[:i], word[i:]) for i in range(len(word) + 1)]\
   deletes    = [a + b[1:] for a, b in s if b]\
   transposes = [a + b[1] + b[0] + b[2:] for a, b in s if len(b)>1]\
   replaces   = [a + c + b[1:] for a, b in s for c in alphabet if b]\
   inserts    = [a + c + b     for a, b in s for c in alphabet]\
   return set(deletes + transposes + replaces + inserts)\
\
def known_edits2(word):\
    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\
\
def known(words): return set(w for w in words if w in NWORDS)\
\
def correct(word):\
    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\
    return max(candidates, key=NWORDS.get)\
\
##############\
\
Bayes' Theorem:\
\

\f1 {{\NeXTGraphic Pasted Graphic.tiff \width4320 \height960
}¬}
\f0 \
\
\pard\pardeftab720\sa240

\f2 \cf0 We will say that we are trying to find the correction 
\i c
\i0 , out of all possible corrections, that maximizes the probability of 
\i c
\i0  given the original word 
\i w
\i0 :\
\pard\pardeftab720
\cf0 argmax
\i\fs20 c
\i0\fs24  P(
\i c
\i0 |
\i w
\i0 )\
\
By Bayes' Theorem this is equivalent to:\
\
argmax
\i\fs20 c
\i0\fs24  P(
\i w
\i0 |
\i c
\i0 ) P(
\i c
\i0 ) / P(
\i w
\i0 )\
\
Since P(
\i w
\i0 ) is the same for every possible 
\i c
\i0 , we can ignore it, giving:\
\
argmax
\i\fs20 c
\i0\fs24  P(
\i w
\i0 |
\i c
\i0 ) P(
\i c
\i0 )\
\
There are three parts of this expression. From right to left, we have:\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720
\ls1\ilvl0\cf0 {\listtext	1.	}P(
\i c
\i0 ), the probability that a proposed correction 
\i c
\i0  stands on its own. This is called the 
\b language model
\b0 : think of it as answering the question "how likely is 
\i c
\i0  to appear in an English text?" So P("the") would have a relatively high probability, while P("zxzxzxzyyy") would be near zero.\
{\listtext	2.	}P(
\i w
\i0 |
\i c
\i0 ), the probability that 
\i w
\i0  would be typed in a text when the author meant 
\i c
\i0 . This is the 
\b error model
\b0 : think of it as answering "how likely is it that the author would type 
\i w
\i0  by mistake when 
\i c
\i0  was intended?"\
{\listtext	3.	}argmax
\i\fs20 c
\i0\fs24 , the control mechanism, which says to enumerate all feasible values of 
\i c
\i0 , and then choose the one that gives the best combined probability score.\
}